{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow Tutorial-8 Dataset & Iterator.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Qw-La8OuoiRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuUCS8bDqoRY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset1=tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "# Emits data of 10, 11, 12, 13, 14, (One element at a time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRdRnFqIqv3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset2=tf.data.Dataset.from_tensor_slices((tf.range(30,45,3),tf.range(10,15)))\n",
        "# Emits data of (30, 60), (33, 62), (36, 64), (39, 66), (42, 68)\n",
        "# Emits one tuple at a time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r83-WTbHhRyK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "outputId": "d20f68de-19b6-4323-db95-d7f71b93bbbc"
      },
      "cell_type": "code",
      "source": [
        "dataset3 = tf.data.Dataset.from_tensor_slices((tf.range(10), tf.range(5)))\n",
        "# Dataset not possible as zeroth dimenion is different at 10 and 5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4b7903aac2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Dataset not possible as zeroth dimenion is different at 10 and 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   1440\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    296\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   1888\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 1890\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" % (self,\n\u001b[0;32m--> 264\u001b[0;31m                                                                     other))\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions 10 and 5 are not compatible"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "viKpaiY40EDY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just like from_tensor_slices, this method also accepts individual (or multiple) Numpy (or Tensors) objects. But this method doesn’t support batching of data, i.e all the data will be given out instantly."
      ]
    },
    {
      "metadata": {
        "id": "9gmxrK64wNOX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset1=tf.data.Dataset.from_tensors(tf.range(10))\n",
        "# Emits data of [10, 11, 12, 13, 14]\n",
        "# Holds entire list as one element"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITy6jmSs0DaU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset2 = tf.data.Dataset.from_tensors((tf.range(30, 45, 3), tf.range(60, 70, 2)))\n",
        "# Emits data of ([30, 33, 36, 39, 42], [60, 62, 64, 66, 68])\n",
        "# Holds entire tuple as one element"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NTCTmJxZ0MPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset3 = tf.data.Dataset.from_tensors((tf.range(10), tf.range(5)))\n",
        "# Possible with from_tensors, regardless of zeroth dimension mismatch of constituent elements.\n",
        "# Emits data of ([1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4])\n",
        "# Holds entire tuple as one element"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hFJM9uYj1diT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this method, a generator function is passed as input. This method is useful in cases where you wish to generate the data at runtime and as such no raw data exists with you or in scenarios where your training data is extremely huge and it is not possible to store them in your disk."
      ]
    },
    {
      "metadata": {
        "id": "xizKlILu0V8e",
        "colab_type": "code",
        "outputId": "c3e42fc0-c328-4cad-a2ca-0385efdb6714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "def generator(sequence_type):\n",
        "  if sequence_type==1:\n",
        "    for i in range(5):\n",
        "      yield 10+i\n",
        "  elif sequence_type==2:\n",
        "    for i in range(5):\n",
        "      yield (30+3*i,60+2*i)\n",
        "  else:\n",
        "    for i in range(1,4):\n",
        "      yield (i,['Hi']*i)\n",
        "      \n",
        "dataset1=tf.data.Dataset.from_generator(generator,(tf.int32),args=([1]))\n",
        "# Emits data of 10, 11, 12, 13, 14, (One element at a time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, use\n",
            "    tf.py_function, which takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2AjkAFeX2c1q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset2=tf.data.Dataset.from_generator(generator,(tf.int32,tf.int32),args=([2]))\n",
        "# Emits data of (30, 60), (33, 62), (36, 64), (39, 66), (42, 68)\n",
        "# Emits one tuple at a time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83P99R2Y22zF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset3=tf.data.Dataset.from_generator(generator,(tf.int32,tf.string),args=([3]))\n",
        "# Emits data of (1, ['Hi']), (2, ['Hi', 'Hi']), (3, ['Hi', 'Hi', 'Hi'])\n",
        "# Emits one tuple at a time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GSYswl5-3GUN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "# Create a dataset with data of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "dataset = dataset.repeat(2)\n",
        "# Duplicate the dataset\n",
        "# Data will be [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "dataset = dataset.shuffle(5)\n",
        "# Shuffle the dataset\n",
        "# Assumed shuffling: [3, 0, 7, 9, 4, 2, 5, 0, 1, 7, 5, 9, 4, 6, 2, 8, 6, 8, 1, 3]\n",
        "\n",
        "def map_fn(x):\n",
        "    return x * 3\n",
        "\n",
        "dataset = dataset.map(map_fn)\n",
        "# Same as dataset = dataset.map(lambda x: x * 3)\n",
        "# Multiply each element with 3 using map transformation\n",
        "# Dataset: [9, 0, 21, 27, 12, 6, 15, 0, 3, 21, 15, 27, 12, 18, 6, 24, 18, 24, 3, 9]\n",
        "\n",
        "def filter_fn(x):\n",
        "    return tf.reshape(tf.not_equal(x % 5, 1), [])\n",
        "\n",
        "dataset = dataset.filter(filter_fn)\n",
        "# Same as dataset = dataset.filter(lambda x: tf.reshape(tf.not_equal(x % 5, 1), []))\n",
        "# Filter out all those elements whose modulus 5 returns 1\n",
        "# Dataset: [9, 0, 27, 12, 15, 0, 3, 15, 27, 12, 18, 24, 18, 24, 3, 9]\n",
        "\n",
        "dataset = dataset.batch(4)\n",
        "# Batch at every 4 elements\n",
        "# Dataset: [9, 0, 27, 12], [15, 0, 3, 15], [27, 12, 18, 24], [18, 24, 3, 9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iWfqxhIE_O4x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Ordering of transformation**\n",
        "\n",
        "The ordering of the application of the transformation is very important. Your model may learn differently for the same Dataset but differently ordered transformations."
      ]
    },
    {
      "metadata": {
        "id": "NzekbnE882R9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Ordering #1\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "dataset1 = dataset1.batch(4)\n",
        "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
        "\n",
        "dataset1 = dataset1.repeat(2)\n",
        "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
        "# Notice a 2 element batch in between\n",
        "\n",
        "dataset1 = dataset1.shuffle(4)\n",
        "# Shuffles at batch level.\n",
        "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7]\n",
        "\n",
        "\n",
        "\n",
        "# Ordering #2\n",
        "dataset2 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "dataset2 = dataset2.shuffle(4)\n",
        "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
        "\n",
        "dataset2 = dataset2.repeat(2)\n",
        "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2, 3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
        "\n",
        "dataset2 = dataset2.batch(4)\n",
        "# Dataset: [3, 1, 0, 4], [5, 8, 6, 9], [7, 2, 3, 1], [0, 4, 5, 8], [6, 9, 7, 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DXenwjAmACHS",
        "colab_type": "code",
        "outputId": "57738cf7-580a-47a9-d13c-cb94eeceffed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "X_train, y_train = mnist.train.images, mnist.train.labels\n",
        "X_val, y_val     = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test   = mnist.test.images, mnist.test.labels"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-0aad2973e0d3>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aThR__kdyr-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.slim as slim\n",
        "\n",
        "# LeNet-5 model\n",
        "class Model:\n",
        "    def __init__(self, data_X, data_y):\n",
        "        self.n_class = 10\n",
        "        self._create_architecture(data_X, data_y)\n",
        "\n",
        "    def _create_architecture(self, data_X, data_y):\n",
        "        y_hot = tf.one_hot(data_y, depth = self.n_class)\n",
        "        logits = self._create_model(data_X)\n",
        "        predictions = tf.argmax(logits, 1, output_type = tf.int32)\n",
        "        self.loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_hot, \n",
        "                                                                              logits = logits))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(self.loss)\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, data_y), tf.float32))\n",
        "\n",
        "    def _create_model(self, X):\n",
        "        X1 = X - 0.5\n",
        "        X1 = tf.pad(X1, tf.constant([[0, 0], [2, 2], [2, 2], [0, 0]]))\n",
        "        with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
        "                            weights_initializer = tf.truncated_normal_initializer(0.0, 0.1)):\n",
        "            net = slim.conv2d(X1, 6, [5, 5], padding = 'VALID')\n",
        "            net = slim.max_pool2d(net, [2, 2])\n",
        "            net = slim.conv2d(net, 16, [5, 5], padding = 'VALID')\n",
        "            net = slim.max_pool2d(net, [2, 2])\n",
        "            \n",
        "            net = tf.reshape(net, [-1, 400])\n",
        "            net = slim.fully_connected(net, 120)\n",
        "            net = slim.fully_connected(net, 84)\n",
        "            net = slim.fully_connected(net, self.n_class, activation_fn = None)\n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aVsewkxYzuLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#OVERVIEW OF ITERATOR\n",
        "```\n",
        "# Create dataset and perform transformations on it\n",
        "dataset = << Create Dataset object >>\n",
        "dataset = << Perform transformations on dataset >>\n",
        "\n",
        "# Create iterator\n",
        "iterator = << Create iterator using dataset >>\n",
        "next_batch = iterator.get_next()\n",
        "\n",
        "# Create session\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    try: \n",
        "        # Keep running next_batch till the Dataset is exhausted\n",
        "        while True:\n",
        "            sess.run(next_batch)\n",
        "            \n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fLppuREVzeA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " iterator doesn’t keep track of how many elements are present in the Dataset. Hence, it is normal to keep running the iterator’s get_next operation till Tensorflow’s tf.errors.OutOfRangeError exception is occurred."
      ]
    },
    {
      "metadata": {
        "id": "ry6n4hj12SlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#One-shot iterator\n",
        "This is the most basic type of iterator. All the data with all types of transformations that is needed in the dataset has to be decided before the Dataset is fed into this iterator. One-shot iterator will iterate through all the elements present in Dataset and once exhausted, cannot be used anymore. As a result, the Dataset generated for this iterator can tend to occupy a lot of memory."
      ]
    },
    {
      "metadata": {
        "id": "6-HwdfBx5QDA",
        "colab_type": "code",
        "outputId": "3865bb53-9bb9-455a-86e2-eec294e19ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4fkZ8o6gzQ6a",
        "colab_type": "code",
        "outputId": "87613abe-057c-4b25-f111-13ef68a56d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "epochs=10\n",
        "batch_size=64\n",
        "iterations=len(y_train)*epochs\n",
        "\n",
        "dataset=tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
        "# Generate the complete Dataset required in the pipeline\n",
        "\n",
        "dataset=dataset.repeat(epochs).batch(batch_size)\n",
        "iterator=dataset.make_one_shot_iterator()\n",
        "\n",
        "data_X,data_y=iterator.get_next()\n",
        "data_y=tf.cast(data_y,tf.int32)\n",
        "model=Model(data_X,data_y)\n",
        "\n",
        "with tf.Session() as sess, tqdm(total=iterations) as pbar:#Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable), and you’re done!\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  tot_accuracy=0\n",
        "  try:\n",
        "    while True:\n",
        "      accuracy, _ = sess.run([model.accuracy, model.optimizer])\n",
        "      tot_accuracy += accuracy\n",
        "      pbar.update(batch_size)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "      \n",
        "print('\\nAverage training accuracy: {:.4f}'.format(tot_accuracy / iterations))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "550016it [06:37, 1403.65it/s]                            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Average training accuracy: 0.9820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pQE-QAN-A6qx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initializable\n",
        "In One-shot iterator, we had the shortfall of repetition of same training dataset in memory and there was absence of periodically validating our model using validation dataset in our code. In initializable iterator we overcome these problems. Initializable iterator has to be initialized with dataset before it starts running."
      ]
    },
    {
      "metadata": {
        "id": "B2LWnZjl4xAN",
        "colab_type": "code",
        "outputId": "36fd925b-dd2a-4291-9d4b-350d49dc88bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "epochs=10\n",
        "batch_size=64\n",
        "iterations=len(y_train)*epochs\n",
        "\n",
        "placeholder_X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "placeholder_y = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
        "dataset = dataset.batch(batch_size)\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "\n",
        "data_X, data_y = iterator.get_next()\n",
        "data_y = tf.cast(data_y, tf.int32)\n",
        "model = Model(data_X, data_y)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch_no in range(epochs):\n",
        "        train_loss, train_accuracy = 0, 0\n",
        "        val_loss, val_accuracy = 0, 0\n",
        "\n",
        "        # Initialize iterator with training data\n",
        "        sess.run(iterator.initializer, feed_dict = {placeholder_X: X_train, placeholder_y: y_train})\n",
        "        try:\n",
        "            with tqdm(total = len(y_train)) as pbar:\n",
        "                while True:\n",
        "                    _, loss, acc = sess.run([model.optimizer, model.loss, model.accuracy])\n",
        "                    train_loss += loss \n",
        "                    train_accuracy += acc\n",
        "                    pbar.update(batch_size) #Manually update the progress bar, useful for streams\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "    \n",
        "        # Initialize iterator with validation data\n",
        "        sess.run(iterator.initializer, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
        "        try:\n",
        "            while True:\n",
        "                loss, acc = sess.run([model.loss, model.accuracy])\n",
        "                val_loss += loss \n",
        "                val_accuracy += acc\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "    \n",
        "        print('\\nEpoch No: {}'.format(epoch_no + 1))\n",
        "        print('Train accuracy = {:.4f}, loss = {:.4f}'.format(train_accuracy / len(y_train), \n",
        "                                                        train_loss / len(y_train)))\n",
        "        print('Val accuracy = {:.4f}, loss = {:.4f}'.format(val_accuracy / len(y_val), \n",
        "                                                        val_loss / len(y_val)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55040it [00:40, 1357.73it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 1\n",
            "Train accuracy = 0.9226, loss = 0.2502\n",
            "Val accuracy = 0.9670, loss = 0.1137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1376.83it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 2\n",
            "Train accuracy = 0.9771, loss = 0.0731\n",
            "Val accuracy = 0.9784, loss = 0.0657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:40, 1362.87it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 3\n",
            "Train accuracy = 0.9843, loss = 0.0505\n",
            "Val accuracy = 0.9834, loss = 0.0538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:40, 1366.67it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 4\n",
            "Train accuracy = 0.9881, loss = 0.0384\n",
            "Val accuracy = 0.9852, loss = 0.0529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:40, 1363.63it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 5\n",
            "Train accuracy = 0.9913, loss = 0.0289\n",
            "Val accuracy = 0.9860, loss = 0.0522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:40, 1372.44it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 6\n",
            "Train accuracy = 0.9928, loss = 0.0234\n",
            "Val accuracy = 0.9848, loss = 0.0527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:40, 1375.96it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 7\n",
            "Train accuracy = 0.9941, loss = 0.0196\n",
            "Val accuracy = 0.9856, loss = 0.0609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:40, 1374.96it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 8\n",
            "Train accuracy = 0.9943, loss = 0.0172\n",
            "Val accuracy = 0.9854, loss = 0.0592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1458.33it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 9\n",
            "Train accuracy = 0.9950, loss = 0.0154\n",
            "Val accuracy = 0.9856, loss = 0.0579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1385.50it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch No: 10\n",
            "Train accuracy = 0.9960, loss = 0.0118\n",
            "Val accuracy = 0.9868, loss = 0.0589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pRisKyo6QXRE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Reinitializable\n",
        "In initializable iterator, there was a shortfall of different datasets undergoing the same pipeline before the Dataset is fed into the iterator. This problem is overcome by reinitializable iterator as we have the ability to feed different types of Datasets thereby undergoing different pipelines. Only one care has to be taken is that different Datasets are of the same data type"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dRdkhcKujCSx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        },
        "outputId": "69693c53-c681-4cea-b422-e303eea33e61"
      },
      "cell_type": "code",
      "source": [
        "def map_fn(x, y):\n",
        "    x=x*3.0+62.0\n",
        "    return x, y\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "placeholder_X = tf.placeholder(tf.float32, shape = [None, 28, 28, 1])\n",
        "placeholder_y = tf.placeholder(tf.int32, shape = [None])\n",
        "\n",
        "# Create separate Datasets for training and validation\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
        "train_dataset = train_dataset.batch(batch_size).map(map_fn)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# Iterator has to have same output types across all Datasets to be used\n",
        "iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
        "data_X, data_y = iterator.get_next()\n",
        "data_y = tf.cast(data_y, tf.int32)\n",
        "model = Model(data_X, data_y)\n",
        "\n",
        "# Initialize with required Datasets\n",
        "train_iterator = iterator.make_initializer(train_dataset)\n",
        "val_iterator = iterator.make_initializer(val_dataset)\n",
        "\n",
        "with tf.Session() as sess:#, tqdm(total=len(y_train)) as pbar::\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_no in range(epochs):\n",
        "        train_loss, train_accuracy = 0, 0\n",
        "        val_loss, val_accuracy = 0, 0\n",
        "\n",
        "        # Start train iterator\n",
        "        sess.run(train_iterator, feed_dict = {placeholder_X: X_train, placeholder_y: y_train})\n",
        "        try:\n",
        "            with tqdm(total = len(y_train)) as pbar:\n",
        "                while True:\n",
        "                    _, acc, loss = sess.run([model.optimizer, model.accuracy, model.loss])\n",
        "                    train_loss += loss\n",
        "                    train_accuracy += acc\n",
        "                    pbar.update(batch_size)\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "\n",
        "        # Start validation iterator\n",
        "        sess.run(val_iterator, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
        "        try:\n",
        "            while True:\n",
        "                acc, loss = sess.run([model.accuracy, model.loss])\n",
        "                val_loss += loss\n",
        "                val_accuracy += acc\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "\n",
        "        print('\\nEpoch: {}'.format(epoch_no + 1))\n",
        "        print('Train accuracy: {:.4f}, loss: {:.4f}'.format(train_accuracy / len(y_train),\n",
        "                                                             train_loss / len(y_train)))\n",
        "        print('Val accuracy: {:.4f}, loss: {:.4f}\\n'.format(val_accuracy / len(y_val), \n",
        "                                                            val_loss / len(y_val)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1400.85it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Train accuracy: 0.7831, loss: 0.7269\n",
            "Val accuracy: 0.2474, loss: 2.6010\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:38, 1435.86it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2\n",
            "Train accuracy: 0.9271, loss: 0.2346\n",
            "Val accuracy: 0.2870, loss: 2.2820\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:38, 1425.60it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3\n",
            "Train accuracy: 0.9513, loss: 0.1575\n",
            "Val accuracy: 0.2962, loss: 2.1088\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:38, 1414.60it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4\n",
            "Train accuracy: 0.9610, loss: 0.1241\n",
            "Val accuracy: 0.3652, loss: 1.8208\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1404.82it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 5\n",
            "Train accuracy: 0.9678, loss: 0.1067\n",
            "Val accuracy: 0.3698, loss: 1.7952\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1405.75it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 6\n",
            "Train accuracy: 0.9729, loss: 0.0876\n",
            "Val accuracy: 0.3692, loss: 1.7915\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1395.43it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 7\n",
            "Train accuracy: 0.9765, loss: 0.0768\n",
            "Val accuracy: 0.3998, loss: 1.7029\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1401.88it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 8\n",
            "Train accuracy: 0.9782, loss: 0.0697\n",
            "Val accuracy: 0.4804, loss: 1.5795\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1398.81it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 9\n",
            "Train accuracy: 0.9800, loss: 0.0645\n",
            "Val accuracy: 0.5110, loss: 1.5920\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "55040it [00:39, 1404.41it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 10\n",
            "Train accuracy: 0.9831, loss: 0.0547\n",
            "Val accuracy: 0.4642, loss: 1.5961\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bt7Rc8J5b-b2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**comparing tqdm position from above and below is, in above after each epoch you can see a progress bar but in below only for 1st epoch you can see a progress bar**"
      ]
    }
  ]
}