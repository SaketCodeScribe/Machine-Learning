{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qw-La8OuoiRi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuUCS8bDqoRY"
   },
   "outputs": [],
   "source": [
    "dataset1=tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Emits data of 10, 11, 12, 13, 14, (One element at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRdRnFqIqv3v"
   },
   "outputs": [],
   "source": [
    "dataset2=tf.data.Dataset.from_tensor_slices((tf.range(30,45,3),tf.range(10,15)))\n",
    "# Emits data of (30, 60), (33, 62), (36, 64), (39, 66), (42, 68)\n",
    "# Emits one tuple at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = tf.data.Dataset.from_tensor_slices((tf.range(10), tf.range(5)))\n",
    "# Dataset not possible as zeroth dimenion is different at 10 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viKpaiY40EDY"
   },
   "source": [
    "Just like from_tensor_slices, this method also accepts individual (or multiple) Numpy (or Tensors) objects. But this method doesnâ€™t support batching of data, i.e all the data will be given out instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9gmxrK64wNOX"
   },
   "outputs": [],
   "source": [
    "dataset1=tf.data.Dataset.from_tensors(tf.range(10))\n",
    "# Emits data of [10, 11, 12, 13, 14]\n",
    "# Holds entire list as one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITy6jmSs0DaU"
   },
   "outputs": [],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensors((tf.range(30, 45, 3), tf.range(60, 70, 2)))\n",
    "# Emits data of ([30, 33, 36, 39, 42], [60, 62, 64, 66, 68])\n",
    "# Holds entire tuple as one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTCTmJxZ0MPR"
   },
   "outputs": [],
   "source": [
    "dataset3 = tf.data.Dataset.from_tensors((tf.range(10), tf.range(5)))\n",
    "# Possible with from_tensors, regardless of zeroth dimension mismatch of constituent elements.\n",
    "# Emits data of ([1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4])\n",
    "# Holds entire tuple as one element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFJM9uYj1diT"
   },
   "source": [
    "In this method, a generator function is passed as input. This method is useful in cases where you wish to generate the data at runtime and as such no raw data exists with you or in scenarios where your training data is extremely huge and it is not possible to store them in your disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "xizKlILu0V8e",
    "outputId": "bf48ed42-9ea6-4f4d-b859-4ff64656a2e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def generator(sequence_type):\n",
    "  if sequence_type==1:\n",
    "    for i in range(5):\n",
    "      yield 10+i\n",
    "  elif sequence_type==2:\n",
    "    for i in range(5):\n",
    "      yield (30+3*i,60+2*i)\n",
    "  else:\n",
    "    for i in range(1,4):\n",
    "      yield (i,['Hi']*i)\n",
    "      \n",
    "dataset1=tf.data.Dataset.from_generator(generator,(tf.int32),args=([1]))\n",
    "# Emits data of 10, 11, 12, 13, 14, (One element at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AjkAFeX2c1q"
   },
   "outputs": [],
   "source": [
    "dataset2=tf.data.Dataset.from_generator(generator,(tf.int32,tf.int32),args=([2]))\n",
    "# Emits data of (30, 60), (33, 62), (36, 64), (39, 66), (42, 68)\n",
    "# Emits one tuple at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83P99R2Y22zF"
   },
   "outputs": [],
   "source": [
    "dataset3=tf.data.Dataset.from_generator(generator,(tf.int32,tf.string),args=([3]))\n",
    "# Emits data of (1, ['Hi']), (2, ['Hi', 'Hi']), (3, ['Hi', 'Hi', 'Hi'])\n",
    "# Emits one tuple at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSYswl5-3GUN"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Create a dataset with data of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset = dataset.repeat(2)\n",
    "# Duplicate the dataset\n",
    "# Data will be [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset = dataset.shuffle(5)\n",
    "# Shuffle the dataset\n",
    "# Assumed shuffling: [3, 0, 7, 9, 4, 2, 5, 0, 1, 7, 5, 9, 4, 6, 2, 8, 6, 8, 1, 3]\n",
    "\n",
    "def map_fn(x):\n",
    "    return x * 3\n",
    "\n",
    "dataset = dataset.map(map_fn)\n",
    "# Same as dataset = dataset.map(lambda x: x * 3)\n",
    "# Multiply each element with 3 using map transformation\n",
    "# Dataset: [9, 0, 21, 27, 12, 6, 15, 0, 3, 21, 15, 27, 12, 18, 6, 24, 18, 24, 3, 9]\n",
    "\n",
    "def filter_fn(x):\n",
    "    return tf.reshape(tf.not_equal(x % 5, 1), [])\n",
    "\n",
    "dataset = dataset.filter(filter_fn)\n",
    "# Same as dataset = dataset.filter(lambda x: tf.reshape(tf.not_equal(x % 5, 1), []))\n",
    "# Filter out all those elements whose modulus 5 returns 1\n",
    "# Dataset: [9, 0, 27, 12, 15, 0, 3, 15, 27, 12, 18, 24, 18, 24, 3, 9]\n",
    "\n",
    "dataset = dataset.batch(4)\n",
    "# Batch at every 4 elements\n",
    "# Dataset: [9, 0, 27, 12], [15, 0, 3, 15], [27, 12, 18, 24], [18, 24, 3, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWfqxhIE_O4x"
   },
   "source": [
    "**Ordering of transformation**\n",
    "\n",
    "The ordering of the application of the transformation is very important. Your model may learn differently for the same Dataset but differently ordered transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzekbnE882R9"
   },
   "outputs": [],
   "source": [
    "# Ordering #1\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset1 = dataset1.batch(4)\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
    "\n",
    "dataset1 = dataset1.repeat(2)\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
    "# Notice a 2 element batch in between\n",
    "\n",
    "dataset1 = dataset1.shuffle(4)\n",
    "# Shuffles at batch level.\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7]\n",
    "\n",
    "\n",
    "\n",
    "# Ordering #2\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset2 = dataset2.shuffle(4)\n",
    "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
    "\n",
    "dataset2 = dataset2.repeat(2)\n",
    "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2, 3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
    "\n",
    "dataset2 = dataset2.batch(4)\n",
    "# Dataset: [3, 1, 0, 4], [5, 8, 6, 9], [7, 2, 3, 1], [0, 4, 5, 8], [6, 9, 7, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "DXenwjAmACHS",
    "outputId": "1ae16c70-f5d9-4f8b-8a4e-621fb8db889b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-0aad2973e0d3>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val     = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test   = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aThR__kdyr-v"
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "# LeNet-5 model\n",
    "class Model:\n",
    "    def __init__(self, data_X, data_y):\n",
    "        self.n_class = 10\n",
    "        self._create_architecture(data_X, data_y)\n",
    "\n",
    "    def _create_architecture(self, data_X, data_y):\n",
    "        y_hot = tf.one_hot(data_y, depth = self.n_class)\n",
    "        logits = self._create_model(data_X)\n",
    "        predictions = tf.argmax(logits, 1, output_type = tf.int32)\n",
    "        self.loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_hot, \n",
    "                                                                              logits = logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(self.loss)\n",
    "        self.accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, data_y), tf.float32))\n",
    "\n",
    "    def _create_model(self, X):\n",
    "        X1 = X - 0.5\n",
    "        X1 = tf.pad(X1, tf.constant([[0, 0], [2, 2], [2, 2], [0, 0]]))\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                            weights_initializer = tf.truncated_normal_initializer(0.0, 0.1)):\n",
    "            net = slim.conv2d(X1, 6, [5, 5], padding = 'VALID')\n",
    "            net = slim.max_pool2d(net, [2, 2])\n",
    "            net = slim.conv2d(net, 16, [5, 5], padding = 'VALID')\n",
    "            net = slim.max_pool2d(net, [2, 2])\n",
    "            \n",
    "            net = tf.reshape(net, [-1, 400])\n",
    "            net = slim.fully_connected(net, 120)\n",
    "            net = slim.fully_connected(net, 84)\n",
    "            net = slim.fully_connected(net, self.n_class, activation_fn = None)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVsewkxYzuLQ"
   },
   "source": [
    "#OVERVIEW OF ITERATOR\n",
    "```\n",
    "# Create dataset and perform transformations on it\n",
    "dataset = << Create Dataset object >>\n",
    "dataset = << Perform transformations on dataset >>\n",
    "\n",
    "# Create iterator\n",
    "iterator = << Create iterator using dataset >>\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "# Create session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try: \n",
    "        # Keep running next_batch till the Dataset is exhausted\n",
    "        while True:\n",
    "            sess.run(next_batch)\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLppuREVzeA4"
   },
   "source": [
    " iterator doesnâ€™t keep track of how many elements are present in the Dataset. Hence, it is normal to keep running the iteratorâ€™s get_next operation till Tensorflowâ€™s tf.errors.OutOfRangeError exception is occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ry6n4hj12SlB"
   },
   "source": [
    "#One-shot iterator\n",
    "This is the most basic type of iterator. All the data with all types of transformations that is needed in the dataset has to be decided before the Dataset is fed into this iterator. One-shot iterator will iterate through all the elements present in Dataset and once exhausted, cannot be used anymore. As a result, the Dataset generated for this iterator can tend to occupy a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6-HwdfBx5QDA",
    "outputId": "d1170f2c-0f76-4602-f94d-40cd81efb3ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "4fkZ8o6gzQ6a",
    "outputId": "8f19de54-2359-41ca-d830-c4994b5fc80f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550016it [02:17, 3987.09it/s]                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs=10\n",
    "batch_size=64\n",
    "iterations=len(y_train)*epochs\n",
    "\n",
    "dataset=tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "# Generate the complete Dataset required in the pipeline\n",
    "\n",
    "dataset=dataset.repeat(epochs).batch(batch_size)\n",
    "iterator=dataset.make_one_shot_iterator()\n",
    "\n",
    "data_X,data_y=iterator.get_next()\n",
    "data_y=tf.cast(data_y,tf.int32)\n",
    "model=Model(data_X,data_y)\n",
    "\n",
    "with tf.Session() as sess, tqdm(total=iterations) as pbar:#Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable), and youâ€™re done!\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tot_accuracy=0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy, _ = sess.run([model.accuracy, model.optimizer])\n",
    "            tot_accuracy += accuracy\n",
    "            pbar.update(batch_size)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "      \n",
    "print('\\nAverage training accuracy: {:.4f}'.format(tot_accuracy / iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQE-QAN-A6qx"
   },
   "source": [
    "# Initializable\n",
    "In One-shot iterator, we had the shortfall of repetition of same training dataset in memory and there was absence of periodically validating our model using validation dataset in our code. In initializable iterator we overcome these problems. Initializable iterator has to be initialized with dataset before it starts running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "B2LWnZjl4xAN",
    "outputId": "e29f0623-86a4-46a8-b8d6-32286be71a1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:09, 5702.16it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 1\n",
      "Train accuracy = 0.9232, loss = 0.2610\n",
      "Val accuracy = 0.9592, loss = 0.1424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:06, 7961.73it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 2\n",
      "Train accuracy = 0.9764, loss = 0.0769\n",
      "Val accuracy = 0.9724, loss = 0.0969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:06, 7884.24it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 3\n",
      "Train accuracy = 0.9836, loss = 0.0537\n",
      "Val accuracy = 0.9784, loss = 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7526.51it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 4\n",
      "Train accuracy = 0.9873, loss = 0.0408\n",
      "Val accuracy = 0.9812, loss = 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7776.25it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 5\n",
      "Train accuracy = 0.9903, loss = 0.0319\n",
      "Val accuracy = 0.9858, loss = 0.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:06, 7952.54it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 6\n",
      "Train accuracy = 0.9922, loss = 0.0262\n",
      "Val accuracy = 0.9888, loss = 0.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7736.92it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 7\n",
      "Train accuracy = 0.9936, loss = 0.0216\n",
      "Val accuracy = 0.9880, loss = 0.0487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7806.02it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 8\n",
      "Train accuracy = 0.9940, loss = 0.0187\n",
      "Val accuracy = 0.9878, loss = 0.0528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7653.06it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 9\n",
      "Train accuracy = 0.9939, loss = 0.0183\n",
      "Val accuracy = 0.9902, loss = 0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7699.05it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch No: 10\n",
      "Train accuracy = 0.9955, loss = 0.0145\n",
      "Val accuracy = 0.9882, loss = 0.0487\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "batch_size=64\n",
    "iterations=len(y_train)*epochs\n",
    "\n",
    "placeholder_X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "placeholder_y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "data_X, data_y = iterator.get_next()\n",
    "data_y = tf.cast(data_y, tf.int32)\n",
    "model = Model(data_X, data_y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_no in range(epochs):\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "\n",
    "        # Initialize iterator with training data\n",
    "        sess.run(iterator.initializer, feed_dict = {placeholder_X: X_train, placeholder_y: y_train})\n",
    "        try:\n",
    "            with tqdm(total = len(y_train)) as pbar:\n",
    "                while True:\n",
    "                    _, loss, acc = sess.run([model.optimizer, model.loss, model.accuracy])\n",
    "                    train_loss += loss \n",
    "                    train_accuracy += acc\n",
    "                    pbar.update(batch_size) #Manually update the progress bar, useful for streams\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "    \n",
    "        # Initialize iterator with validation data\n",
    "        sess.run(iterator.initializer, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
    "        try:\n",
    "            while True:\n",
    "                loss, acc = sess.run([model.loss, model.accuracy])\n",
    "                val_loss += loss \n",
    "                val_accuracy += acc\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "    \n",
    "        print('\\nEpoch No: {}'.format(epoch_no + 1))\n",
    "        print('Train accuracy = {:.4f}, loss = {:.4f}'.format(train_accuracy / len(y_train), \n",
    "                                                        train_loss / len(y_train)))\n",
    "        print('Val accuracy = {:.4f}, loss = {:.4f}'.format(val_accuracy / len(y_val), \n",
    "                                                        val_loss / len(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pRisKyo6QXRE"
   },
   "source": [
    "#Reinitializable\n",
    "In initializable iterator, there was a shortfall of different datasets undergoing the same pipeline before the Dataset is fed into the iterator. This problem is overcome by reinitializable iterator as we have the ability to feed different types of Datasets thereby undergoing different pipelines. Only one care has to be taken is that different Datasets are of the same data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRdkhcKujCSx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:08, 6155.97it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "Train accuracy: 0.7415, loss: 0.8195\n",
      "Val accuracy: 0.4628, loss: 1.6218\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7438.07it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9357, loss: 0.2060\n",
      "Val accuracy: 0.6182, loss: 1.1918\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7714.17it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9583, loss: 0.1331\n",
      "Val accuracy: 0.7546, loss: 0.9779\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7633.97it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9662, loss: 0.1042\n",
      "Val accuracy: 0.7558, loss: 0.9080\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7671.16it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9728, loss: 0.0869\n",
      "Val accuracy: 0.7886, loss: 0.8332\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7673.30it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9776, loss: 0.0717\n",
      "Val accuracy: 0.8134, loss: 0.7441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7444.10it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9785, loss: 0.0667\n",
      "Val accuracy: 0.8156, loss: 0.7388\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7704.42it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9815, loss: 0.0565\n",
      "Val accuracy: 0.7874, loss: 0.7514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:06, 7867.33it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9827, loss: 0.0520\n",
      "Val accuracy: 0.8062, loss: 0.6939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55040it [00:07, 7804.89it/s]                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10\n",
      "Train accuracy: 0.9857, loss: 0.0439\n",
      "Val accuracy: 0.8034, loss: 0.7030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_fn(x, y):\n",
    "    x=x*3.0+62.0\n",
    "    return x, y\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "placeholder_X = tf.placeholder(tf.float32, shape = [None, 28, 28, 1])\n",
    "placeholder_y = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "# Create separate Datasets for training and validation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "train_dataset = train_dataset.batch(batch_size).map(map_fn)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Iterator has to have same output types across all Datasets to be used\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "data_X, data_y = iterator.get_next()\n",
    "data_y = tf.cast(data_y, tf.int32)\n",
    "model = Model(data_X, data_y)\n",
    "\n",
    "# Initialize with required Datasets\n",
    "train_iterator = iterator.make_initializer(train_dataset)\n",
    "val_iterator = iterator.make_initializer(val_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_no in range(epochs):\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "\n",
    "        # Start train iterator\n",
    "        sess.run(train_iterator, feed_dict = {placeholder_X: X_train, placeholder_y: y_train})\n",
    "        try:\n",
    "            with tqdm(total = len(y_train)) as pbar:\n",
    "                while True:\n",
    "                    _, acc, loss = sess.run([model.optimizer, model.accuracy, model.loss])\n",
    "                    train_loss += loss\n",
    "                    train_accuracy += acc\n",
    "                    pbar.update(batch_size)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        # Start validation iterator\n",
    "        sess.run(val_iterator, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
    "        try:\n",
    "            while True:\n",
    "                acc, loss = sess.run([model.accuracy, model.loss])\n",
    "                val_loss += loss\n",
    "                val_accuracy += acc\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('\\nEpoch: {}'.format(epoch_no + 1))\n",
    "        print('Train accuracy: {:.4f}, loss: {:.4f}'.format(train_accuracy / len(y_train),\n",
    "                                                             train_loss / len(y_train)))\n",
    "        print('Val accuracy: {:.4f}, loss: {:.4f}\\n'.format(val_accuracy / len(y_val), \n",
    "                                                            val_loss / len(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bt7Rc8J5b-b2"
   },
   "source": [
    "**comparing tqdm position from above and below is in above after each epoch you can see a progress bar but in below only for 1st epoch you can see a progress bar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFhWBLdzXpkM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 54848/55000 [00:13<00:00, 7129.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "Train accuracy: 0.1323, loss: 2.2529\n",
      "Val accuracy: 0.1028, loss: 3.2966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109440it [00:25, 7213.70it/s]                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n",
      "Train accuracy: 0.2219, loss: 1.9559\n",
      "Val accuracy: 0.0894, loss: 4.0944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164864it [00:35, 7810.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3\n",
      "Train accuracy: 0.2319, loss: 1.9323\n",
      "Val accuracy: 0.0964, loss: 5.6891\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219904it [00:44, 7449.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4\n",
      "Train accuracy: 0.2343, loss: 1.9302\n",
      "Val accuracy: 0.0934, loss: 6.5835\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275136it [00:53, 7510.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5\n",
      "Train accuracy: 0.1114, loss: 2.3046\n",
      "Val accuracy: 0.1098, loss: 5.8575\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "329472it [01:03, 7606.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6\n",
      "Train accuracy: 0.1121, loss: 2.3016\n",
      "Val accuracy: 0.1100, loss: 5.8905\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "385024it [01:12, 7355.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7\n",
      "Train accuracy: 0.1123, loss: 2.3015\n",
      "Val accuracy: 0.1098, loss: 5.7503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439616it [01:21, 7545.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8\n",
      "Train accuracy: 0.1122, loss: 2.3014\n",
      "Val accuracy: 0.1098, loss: 5.6159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "494912it [01:30, 7689.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9\n",
      "Train accuracy: 0.1123, loss: 2.3014\n",
      "Val accuracy: 0.1100, loss: 5.5049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550144it [01:40, 7027.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10\n",
      "Train accuracy: 0.1124, loss: 2.3014\n",
      "Val accuracy: 0.1100, loss: 5.4021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550400it [01:41, 5402.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def map_fn(x,y):\n",
    "  x=x*3.0+62.0\n",
    "  return (x,y)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "placeholder_X = tf.placeholder(tf.float32, shape = [None, 28, 28, 1])\n",
    "placeholder_y = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "# Create separate Datasets for training and validation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "train_dataset = train_dataset.batch(batch_size).map(lambda x, y: map_fn(x, y))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Iterator has to have same output types across all Datasets to be used\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "data_X, data_y = iterator.get_next()\n",
    "data_y = tf.cast(data_y, tf.int32)\n",
    "model = Model(data_X, data_y)\n",
    "\n",
    "# Initialize with required Datasets\n",
    "train_iterator = iterator.make_initializer(train_dataset)\n",
    "val_iterator = iterator.make_initializer(val_dataset)\n",
    "\n",
    "with tf.Session() as sess, tqdm(total=len(y_train)) as pbar:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for epoch_no in range(epochs):\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        \n",
    "        # Start train iterator\n",
    "        sess.run(train_iterator,feed_dict={placeholder_X:X_train,placeholder_y:y_train})\n",
    "        try:\n",
    "          while True:\n",
    "            _, acc, loss = sess.run([model.optimizer, model.accuracy, model.loss])\n",
    "            train_loss += loss\n",
    "            train_accuracy += acc\n",
    "            pbar.update(batch_size)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        # Start validation iterator\n",
    "        sess.run(val_iterator, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
    "        try:\n",
    "            while True:\n",
    "                acc, loss = sess.run([model.accuracy, model.loss])\n",
    "                val_loss += loss\n",
    "                val_accuracy += acc\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('\\nEpoch: {}'.format(epoch_no + 1))\n",
    "        print('Train accuracy: {:.4f}, loss: {:.4f}'.format(train_accuracy / len(y_train),\n",
    "                                                             train_loss / len(y_train)))\n",
    "        print('Val accuracy: {:.4f}, loss: {:.4f}\\n'.format(val_accuracy / len(y_val), \n",
    "                                                            val_loss / len(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzVYPnLlYwIn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tensorflow Tutorial-8 Dataset & Iterator.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
